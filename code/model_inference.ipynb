{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: This submission utilizes the maximum inference time limit, so depending on the situation, a submission scoring error may occur. \n",
    "\n",
    "However, you can succeed by trying multiple times.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-21T10:55:38.440216Z",
     "iopub.status.busy": "2024-11-21T10:55:38.439112Z",
     "iopub.status.idle": "2024-11-21T10:55:46.008868Z",
     "shell.execute_reply": "2024-11-21T10:55:46.007902Z",
     "shell.execute_reply.started": "2024-11-21T10:55:38.440164Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "def read_json_file(file_path):\n",
    "    \"\"\"Read a JSON file and parse it into a Python object.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the JSON file to read.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary object representing the JSON data.\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If the specified file path does not exist.\n",
    "        ValueError: If the specified file path does not contain valid JSON data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Open the file and load the JSON data into a Python object\n",
    "        with open(file_path, 'r') as file:\n",
    "            json_data = json.load(file)\n",
    "        return json_data\n",
    "    except FileNotFoundError:\n",
    "        # Raise an error if the file path does not exist\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    except ValueError:\n",
    "        # Raise an error if the file does not contain valid JSON data\n",
    "        raise ValueError(f\"Invalid JSON data in file: {file_path}\")\n",
    "\n",
    "cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:09:54.339342Z",
     "iopub.status.busy": "2024-11-19T09:09:54.338174Z",
     "iopub.status.idle": "2024-11-19T09:09:54.593139Z",
     "shell.execute_reply": "2024-11-19T09:09:54.592117Z",
     "shell.execute_reply.started": "2024-11-19T09:09:54.339311Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "... LOAD SIGN TO PREDICTION INDEX MAP FROM JSON FILE ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "train_df = train_df.loc[(train_df[\"sequence_id\"] == 0)].reset_index(drop = True)\n",
    "print(\"\\n\\n... LOAD SIGN TO PREDICTION INDEX MAP FROM JSON FILE ...\\n\")\n",
    "#s2p_map = {k.lower():v for k,v in read_json_file(os.path.join(\"sign_to_prediction_index_map.json\")).items()}\n",
    "#p2s_map = {v:k for k,v in read_json_file(os.path.join(\"sign_to_prediction_index_map.json\")).items()}\n",
    "s2p_map = {k.lower():v for k,v in read_json_file(os.path.join(\"sign_to_prediction_index_map_merge.json\")).items()}\n",
    "p2s_map = {v:k for k,v in read_json_file(os.path.join(\"sign_to_prediction_index_map_merge.json\")).items()}\n",
    "encoder = lambda x: s2p_map.get(x.lower())\n",
    "decoder = lambda x: p2s_map.get(x)\n",
    "# print(s2p_map)\n",
    "train_df['label'] = train_df.sign.map(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:09:54.594750Z",
     "iopub.status.busy": "2024-11-19T09:09:54.594390Z",
     "iopub.status.idle": "2024-11-19T09:09:54.615195Z",
     "shell.execute_reply": "2024-11-19T09:09:54.614079Z",
     "shell.execute_reply.started": "2024-11-19T09:09:54.594722Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118\n",
      "708\n"
     ]
    }
   ],
   "source": [
    "ROWS_PER_FRAME = 543\n",
    "MAX_LEN = 384\n",
    "CROP_LEN = MAX_LEN\n",
    "#NUM_CLASSES  = 250\n",
    "NUM_CLASSES  = 26\n",
    "PAD = -100.\n",
    "NOSE=[\n",
    "    1,2,98,327\n",
    "]\n",
    "LNOSE = [98]\n",
    "RNOSE = [327]\n",
    "LIP = [ 0, \n",
    "    61, 185, 40, 39, 37, 267, 269, 270, 409,\n",
    "    291, 146, 91, 181, 84, 17, 314, 405, 321, 375,\n",
    "    78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n",
    "    95, 88, 178, 87, 14, 317, 402, 318, 324, 308,\n",
    "]\n",
    "LLIP = [84,181,91,146,61,185,40,39,37,87,178,88,95,78,191,80,81,82]\n",
    "RLIP = [314,405,321,375,291,409,270,269,267,317,402,318,324,308,415,310,311,312]\n",
    "\n",
    "POSE = [500, 502, 504, 501, 503, 505, 512, 513]\n",
    "LPOSE = [513,505,503,501]\n",
    "RPOSE = [512,504,502,500]\n",
    "\n",
    "REYE = [\n",
    "    33, 7, 163, 144, 145, 153, 154, 155, 133,\n",
    "    246, 161, 160, 159, 158, 157, 173,\n",
    "]\n",
    "LEYE = [\n",
    "    263, 249, 390, 373, 374, 380, 381, 382, 362,\n",
    "    466, 388, 387, 386, 385, 384, 398,\n",
    "]\n",
    "\n",
    "LHAND = np.arange(468, 489).tolist()\n",
    "RHAND = np.arange(522, 543).tolist()\n",
    "\n",
    "POINT_LANDMARKS = LIP + LHAND + RHAND + NOSE + REYE + LEYE #+POSE\n",
    "\n",
    "NUM_NODES = len(POINT_LANDMARKS)\n",
    "CHANNELS = 6*NUM_NODES\n",
    "\n",
    "print(NUM_NODES)\n",
    "print(CHANNELS)\n",
    "\n",
    "def tf_nan_mean(x, axis=0, keepdims=False):\n",
    "    return tf.reduce_sum(tf.where(tf.math.is_nan(x), tf.zeros_like(x), x), axis=axis, keepdims=keepdims) / tf.reduce_sum(tf.where(tf.math.is_nan(x), tf.zeros_like(x), tf.ones_like(x)), axis=axis, keepdims=keepdims)\n",
    "\n",
    "def tf_nan_std(x, center=None, axis=0, keepdims=False):\n",
    "    if center is None:\n",
    "        center = tf_nan_mean(x, axis=axis,  keepdims=True)\n",
    "    d = x - center\n",
    "    return tf.math.sqrt(tf_nan_mean(d * d, axis=axis, keepdims=keepdims))\n",
    "\n",
    "class Preprocess(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_len=MAX_LEN, point_landmarks=POINT_LANDMARKS, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.max_len = max_len\n",
    "        self.point_landmarks = point_landmarks\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if tf.rank(inputs) == 3:\n",
    "            x = inputs[None,...]\n",
    "        else:\n",
    "            x = inputs\n",
    "        \n",
    "        mean = tf_nan_mean(tf.gather(x, [17], axis=2), axis=[1,2], keepdims=True)\n",
    "        mean = tf.where(tf.math.is_nan(mean), tf.constant(0.5,x.dtype), mean)\n",
    "        x = tf.gather(x, self.point_landmarks, axis=2) #N,T,P,C\n",
    "        std = tf_nan_std(x, center=mean, axis=[1,2], keepdims=True)\n",
    "        \n",
    "        x = (x - mean)/std\n",
    "\n",
    "        if self.max_len is not None:\n",
    "            x = x[:,:self.max_len]\n",
    "        length = tf.shape(x)[1]\n",
    "        x = x[...,:2]\n",
    "\n",
    "        dx = tf.cond(tf.shape(x)[1]>1,lambda:tf.pad(x[:,1:] - x[:,:-1], [[0,0],[0,1],[0,0],[0,0]]),lambda:tf.zeros_like(x))\n",
    "\n",
    "        dx2 = tf.cond(tf.shape(x)[1]>2,lambda:tf.pad(x[:,2:] - x[:,:-2], [[0,0],[0,2],[0,0],[0,0]]),lambda:tf.zeros_like(x))\n",
    "\n",
    "        x = tf.concat([\n",
    "            tf.reshape(x, (-1,length,2*len(self.point_landmarks))),\n",
    "            tf.reshape(dx, (-1,length,2*len(self.point_landmarks))),\n",
    "            tf.reshape(dx2, (-1,length,2*len(self.point_landmarks))),\n",
    "        ], axis = -1)\n",
    "        \n",
    "        x = tf.where(tf.math.is_nan(x),tf.constant(0.,x.dtype),x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:09:54.617559Z",
     "iopub.status.busy": "2024-11-19T09:09:54.617278Z",
     "iopub.status.idle": "2024-11-19T09:09:54.639191Z",
     "shell.execute_reply": "2024-11-19T09:09:54.638291Z",
     "shell.execute_reply.started": "2024-11-19T09:09:54.617535Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ECA(tf.keras.layers.Layer):\n",
    "    def __init__(self, kernel_size=5, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv = tf.keras.layers.Conv1D(1, kernel_size=kernel_size, strides=1, padding=\"same\", use_bias=False)\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        nn = tf.keras.layers.GlobalAveragePooling1D()(inputs, mask=mask)\n",
    "        nn = tf.expand_dims(nn, -1)\n",
    "        nn = self.conv(nn)\n",
    "        nn = tf.squeeze(nn, -1)\n",
    "        nn = tf.nn.sigmoid(nn)\n",
    "        nn = nn[:,None,:]\n",
    "        return inputs * nn\n",
    "\n",
    "class LateDropout(tf.keras.layers.Layer):\n",
    "    def __init__(self, rate, noise_shape=None, start_step=0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.rate = rate\n",
    "        self.start_step = start_step\n",
    "        self.dropout = tf.keras.layers.Dropout(rate, noise_shape=noise_shape)\n",
    "      \n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)\n",
    "        agg = tf.VariableAggregation.ONLY_FIRST_REPLICA\n",
    "        self._train_counter = tf.Variable(0, dtype=\"int64\", aggregation=agg, trainable=False)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = tf.cond(self._train_counter < self.start_step, lambda:inputs, lambda:self.dropout(inputs, training=training))\n",
    "        if training:\n",
    "            self._train_counter.assign_add(1)\n",
    "        return x\n",
    "\n",
    "class CausalDWConv1D(tf.keras.layers.Layer):\n",
    "    def __init__(self, \n",
    "        kernel_size=17,\n",
    "        dilation_rate=1,\n",
    "        use_bias=False,\n",
    "        depthwise_initializer='glorot_uniform',\n",
    "        name='', **kwargs):\n",
    "        super().__init__(name=name,**kwargs)\n",
    "        self.causal_pad = tf.keras.layers.ZeroPadding1D((dilation_rate*(kernel_size-1),0),name=name + '_pad')\n",
    "        self.dw_conv = tf.keras.layers.DepthwiseConv1D(\n",
    "                            kernel_size,\n",
    "                            strides=1,\n",
    "                            dilation_rate=dilation_rate,\n",
    "                            padding='valid',\n",
    "                            use_bias=use_bias,\n",
    "                            depthwise_initializer=depthwise_initializer,\n",
    "                            name=name + '_dwconv')\n",
    "        self.supports_masking = True\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.causal_pad(inputs)\n",
    "        x = self.dw_conv(x)\n",
    "        return x\n",
    "\n",
    "def Conv1DBlock(channel_size,\n",
    "          kernel_size,\n",
    "          dilation_rate=1,\n",
    "          drop_rate=0.0,\n",
    "          expand_ratio=2,\n",
    "          se_ratio=0.25,\n",
    "          activation='swish',\n",
    "          name=None):\n",
    "    '''\n",
    "    efficient conv1d block, @hoyso48\n",
    "    '''\n",
    "    if name is None:\n",
    "        name = str(tf.keras.backend.get_uid(\"mbblock\"))\n",
    "    # Expansion phase\n",
    "    def apply(inputs):\n",
    "        channels_in = tf.keras.backend.int_shape(inputs)[-1]\n",
    "        channels_expand = channels_in * expand_ratio\n",
    "\n",
    "        skip = inputs\n",
    "\n",
    "        x = tf.keras.layers.Dense(\n",
    "            channels_expand,\n",
    "            use_bias=True,\n",
    "            activation=activation,\n",
    "            name=name + '_expand_conv')(inputs)\n",
    "\n",
    "        # Depthwise Convolution\n",
    "        x = CausalDWConv1D(kernel_size,\n",
    "            dilation_rate=dilation_rate,\n",
    "            use_bias=False,\n",
    "            name=name + '_dwconv')(x)\n",
    "\n",
    "        x = tf.keras.layers.BatchNormalization(momentum=0.95, name=name + '_bn')(x)\n",
    "\n",
    "        x  = ECA()(x)\n",
    "\n",
    "        x = tf.keras.layers.Dense(\n",
    "            channel_size,\n",
    "            use_bias=True,\n",
    "            name=name + '_project_conv')(x)\n",
    "\n",
    "        if drop_rate > 0:\n",
    "            x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1), name=name + '_drop')(x)\n",
    "\n",
    "        if (channels_in == channel_size):\n",
    "            x = tf.keras.layers.add([x, skip], name=name + '_add')\n",
    "        return x\n",
    "\n",
    "    return apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:09:54.640620Z",
     "iopub.status.busy": "2024-11-19T09:09:54.640336Z",
     "iopub.status.idle": "2024-11-19T09:09:54.656844Z",
     "shell.execute_reply": "2024-11-19T09:09:54.656004Z",
     "shell.execute_reply.started": "2024-11-19T09:09:54.640595Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim=256, num_heads=4, dropout=0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dim = dim\n",
    "        self.scale = self.dim ** -0.5\n",
    "        self.num_heads = num_heads\n",
    "        self.qkv = tf.keras.layers.Dense(3 * dim, use_bias=False)\n",
    "        self.drop1 = tf.keras.layers.Dropout(dropout)\n",
    "        self.proj = tf.keras.layers.Dense(dim, use_bias=False)\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        qkv = self.qkv(inputs)\n",
    "        qkv = tf.keras.layers.Permute((2, 1, 3))(tf.keras.layers.Reshape((-1, self.num_heads, self.dim * 3 // self.num_heads))(qkv))\n",
    "        q, k, v = tf.split(qkv, [self.dim // self.num_heads] * 3, axis=-1)\n",
    "\n",
    "        attn = tf.matmul(q, k, transpose_b=True) * self.scale\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask[:, None, None, :]\n",
    "\n",
    "        attn = tf.keras.layers.Softmax(axis=-1)(attn, mask=mask)\n",
    "        attn = self.drop1(attn)\n",
    "\n",
    "        x = attn @ v\n",
    "        x = tf.keras.layers.Reshape((-1, self.dim))(tf.keras.layers.Permute((2, 1, 3))(x))\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def TransformerBlock(dim=256, num_heads=4, expand=4, attn_dropout=0.2, drop_rate=0.2, activation='swish'):\n",
    "    def apply(inputs):\n",
    "        x = inputs\n",
    "        x = tf.keras.layers.BatchNormalization(momentum=0.95)(x)\n",
    "        x = MultiHeadSelfAttention(dim=dim,num_heads=num_heads,dropout=attn_dropout)(x)\n",
    "        x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1))(x)\n",
    "        x = tf.keras.layers.Add()([inputs, x])\n",
    "        attn_out = x\n",
    "\n",
    "        x = tf.keras.layers.BatchNormalization(momentum=0.95)(x)\n",
    "        x = tf.keras.layers.Dense(dim*expand, use_bias=False, activation=activation)(x)\n",
    "        x = tf.keras.layers.Dense(dim, use_bias=False)(x)\n",
    "        x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1))(x)\n",
    "        x = tf.keras.layers.Add()([attn_out, x])\n",
    "        return x\n",
    "    return apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:09:54.658154Z",
     "iopub.status.busy": "2024-11-19T09:09:54.657867Z",
     "iopub.status.idle": "2024-11-19T09:09:54.672905Z",
     "shell.execute_reply": "2024-11-19T09:09:54.671930Z",
     "shell.execute_reply.started": "2024-11-19T09:09:54.658129Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_model(max_len=MAX_LEN, dropout_step=0, dim=192):\n",
    "    inp = tf.keras.Input((max_len,CHANNELS))\n",
    "    #x = tf.keras.layers.Masking(mask_value=PAD,input_shape=(max_len,CHANNELS))(inp) #we don't need masking layer with inference\n",
    "    x = inp\n",
    "    ksize = 17\n",
    "    x = tf.keras.layers.Dense(dim, use_bias=False,name='stem_conv')(x)\n",
    "    x = tf.keras.layers.BatchNormalization(momentum=0.95,name='stem_bn')(x)\n",
    "\n",
    "    x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n",
    "    x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n",
    "    x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n",
    "    x = TransformerBlock(dim,expand=2)(x)\n",
    "\n",
    "    x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n",
    "    x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n",
    "    x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n",
    "    x = TransformerBlock(dim,expand=2)(x)\n",
    "\n",
    "    if dim == 384: #for the 4x sized model\n",
    "        x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n",
    "        x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n",
    "        x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n",
    "        x = TransformerBlock(dim,expand=2)(x)\n",
    "\n",
    "        x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n",
    "        x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n",
    "        x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n",
    "        x = TransformerBlock(dim,expand=2)(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(dim*2,activation=None,name='top_conv')(x)\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "    x = LateDropout(0.8, start_step=dropout_step)(x)\n",
    "    x = tf.keras.layers.Dense(NUM_CLASSES,name='classifier')(x)\n",
    "    return tf.keras.Model(inp, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:09:54.674927Z",
     "iopub.status.busy": "2024-11-19T09:09:54.674274Z",
     "iopub.status.idle": "2024-11-19T09:10:01.280255Z",
     "shell.execute_reply": "2024-11-19T09:10:01.279228Z",
     "shell.execute_reply.started": "2024-11-19T09:09:54.674891Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 384, 708)]   0           []                               \n",
      "                                                                                                  \n",
      " stem_conv (Dense)              (None, 384, 192)     135936      ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " stem_bn (BatchNormalization)   (None, 384, 192)     768         ['stem_conv[0][0]']              \n",
      "                                                                                                  \n",
      " 7_expand_conv (Dense)          (None, 384, 384)     74112       ['stem_bn[0][0]']                \n",
      "                                                                                                  \n",
      " 7_dwconv (CausalDWConv1D)      (None, 384, 384)     6528        ['7_expand_conv[0][0]']          \n",
      "                                                                                                  \n",
      " 7_bn (BatchNormalization)      (None, 384, 384)     1536        ['7_dwconv[0][0]']               \n",
      "                                                                                                  \n",
      " eca_6 (ECA)                    (None, 384, 384)     5           ['7_bn[0][0]']                   \n",
      "                                                                                                  \n",
      " 7_project_conv (Dense)         (None, 384, 192)     73920       ['eca_6[0][0]']                  \n",
      "                                                                                                  \n",
      " 7_drop (Dropout)               (None, 384, 192)     0           ['7_project_conv[0][0]']         \n",
      "                                                                                                  \n",
      " 7_add (Add)                    (None, 384, 192)     0           ['7_drop[0][0]',                 \n",
      "                                                                  'stem_bn[0][0]']                \n",
      "                                                                                                  \n",
      " 8_expand_conv (Dense)          (None, 384, 384)     74112       ['7_add[0][0]']                  \n",
      "                                                                                                  \n",
      " 8_dwconv (CausalDWConv1D)      (None, 384, 384)     6528        ['8_expand_conv[0][0]']          \n",
      "                                                                                                  \n",
      " 8_bn (BatchNormalization)      (None, 384, 384)     1536        ['8_dwconv[0][0]']               \n",
      "                                                                                                  \n",
      " eca_7 (ECA)                    (None, 384, 384)     5           ['8_bn[0][0]']                   \n",
      "                                                                                                  \n",
      " 8_project_conv (Dense)         (None, 384, 192)     73920       ['eca_7[0][0]']                  \n",
      "                                                                                                  \n",
      " 8_drop (Dropout)               (None, 384, 192)     0           ['8_project_conv[0][0]']         \n",
      "                                                                                                  \n",
      " 8_add (Add)                    (None, 384, 192)     0           ['8_drop[0][0]',                 \n",
      "                                                                  '7_add[0][0]']                  \n",
      "                                                                                                  \n",
      " 9_expand_conv (Dense)          (None, 384, 384)     74112       ['8_add[0][0]']                  \n",
      "                                                                                                  \n",
      " 9_dwconv (CausalDWConv1D)      (None, 384, 384)     6528        ['9_expand_conv[0][0]']          \n",
      "                                                                                                  \n",
      " 9_bn (BatchNormalization)      (None, 384, 384)     1536        ['9_dwconv[0][0]']               \n",
      "                                                                                                  \n",
      " eca_8 (ECA)                    (None, 384, 384)     5           ['9_bn[0][0]']                   \n",
      "                                                                                                  \n",
      " 9_project_conv (Dense)         (None, 384, 192)     73920       ['eca_8[0][0]']                  \n",
      "                                                                                                  \n",
      " 9_drop (Dropout)               (None, 384, 192)     0           ['9_project_conv[0][0]']         \n",
      "                                                                                                  \n",
      " 9_add (Add)                    (None, 384, 192)     0           ['9_drop[0][0]',                 \n",
      "                                                                  '8_add[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 384, 192)    768         ['9_add[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_self_attention_2 (M  (None, 384, 192)    147456      ['batch_normalization_4[0][0]']  \n",
      " ultiHeadSelfAttention)                                                                           \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 384, 192)     0           ['multi_head_self_attention_2[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 384, 192)     0           ['9_add[0][0]',                  \n",
      "                                                                  'dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 384, 192)    768         ['add_4[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 384, 384)     73728       ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 384, 192)     73728       ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 384, 192)     0           ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 384, 192)     0           ['add_4[0][0]',                  \n",
      "                                                                  'dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " 10_expand_conv (Dense)         (None, 384, 384)     74112       ['add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " 10_dwconv (CausalDWConv1D)     (None, 384, 384)     6528        ['10_expand_conv[0][0]']         \n",
      "                                                                                                  \n",
      " 10_bn (BatchNormalization)     (None, 384, 384)     1536        ['10_dwconv[0][0]']              \n",
      "                                                                                                  \n",
      " eca_9 (ECA)                    (None, 384, 384)     5           ['10_bn[0][0]']                  \n",
      "                                                                                                  \n",
      " 10_project_conv (Dense)        (None, 384, 192)     73920       ['eca_9[0][0]']                  \n",
      "                                                                                                  \n",
      " 10_drop (Dropout)              (None, 384, 192)     0           ['10_project_conv[0][0]']        \n",
      "                                                                                                  \n",
      " 10_add (Add)                   (None, 384, 192)     0           ['10_drop[0][0]',                \n",
      "                                                                  'add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " 11_expand_conv (Dense)         (None, 384, 384)     74112       ['10_add[0][0]']                 \n",
      "                                                                                                  \n",
      " 11_dwconv (CausalDWConv1D)     (None, 384, 384)     6528        ['11_expand_conv[0][0]']         \n",
      "                                                                                                  \n",
      " 11_bn (BatchNormalization)     (None, 384, 384)     1536        ['11_dwconv[0][0]']              \n",
      "                                                                                                  \n",
      " eca_10 (ECA)                   (None, 384, 384)     5           ['11_bn[0][0]']                  \n",
      "                                                                                                  \n",
      " 11_project_conv (Dense)        (None, 384, 192)     73920       ['eca_10[0][0]']                 \n",
      "                                                                                                  \n",
      " 11_drop (Dropout)              (None, 384, 192)     0           ['11_project_conv[0][0]']        \n",
      "                                                                                                  \n",
      " 11_add (Add)                   (None, 384, 192)     0           ['11_drop[0][0]',                \n",
      "                                                                  '10_add[0][0]']                 \n",
      "                                                                                                  \n",
      " 12_expand_conv (Dense)         (None, 384, 384)     74112       ['11_add[0][0]']                 \n",
      "                                                                                                  \n",
      " 12_dwconv (CausalDWConv1D)     (None, 384, 384)     6528        ['12_expand_conv[0][0]']         \n",
      "                                                                                                  \n",
      " 12_bn (BatchNormalization)     (None, 384, 384)     1536        ['12_dwconv[0][0]']              \n",
      "                                                                                                  \n",
      " eca_11 (ECA)                   (None, 384, 384)     5           ['12_bn[0][0]']                  \n",
      "                                                                                                  \n",
      " 12_project_conv (Dense)        (None, 384, 192)     73920       ['eca_11[0][0]']                 \n",
      "                                                                                                  \n",
      " 12_drop (Dropout)              (None, 384, 192)     0           ['12_project_conv[0][0]']        \n",
      "                                                                                                  \n",
      " 12_add (Add)                   (None, 384, 192)     0           ['12_drop[0][0]',                \n",
      "                                                                  '11_add[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 384, 192)    768         ['12_add[0][0]']                 \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_self_attention_3 (M  (None, 384, 192)    147456      ['batch_normalization_6[0][0]']  \n",
      " ultiHeadSelfAttention)                                                                           \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 384, 192)     0           ['multi_head_self_attention_3[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 384, 192)     0           ['12_add[0][0]',                 \n",
      "                                                                  'dropout_11[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 384, 192)    768         ['add_6[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 384, 384)     73728       ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 384, 192)     73728       ['dense_14[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 384, 192)     0           ['dense_15[0][0]']               \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 384, 192)     0           ['add_6[0][0]',                  \n",
      "                                                                  'dropout_12[0][0]']             \n",
      "                                                                                                  \n",
      " top_conv (Dense)               (None, 384, 384)     74112       ['add_7[0][0]']                  \n",
      "                                                                                                  \n",
      " global_average_pooling1d_1 (Gl  (None, 384)         0           ['top_conv[0][0]']               \n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " late_dropout_1 (LateDropout)   (None, 384)          1           ['global_average_pooling1d_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " classifier (Dense)             (None, 26)           10010       ['late_dropout_1[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,750,329\n",
      "Trainable params: 1,743,800\n",
      "Non-trainable params: 6,529\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "models_path = ['model/fslr-fp16-192-8-seed42-fold0-best.h5'\n",
    "              #'model/islr-fp16-192-8-seed42-foldall-last.h5', #comment out other weights to check single model score\n",
    "               #'model/islr-fp16-192-8-seed43-foldall-last.h5',\n",
    "               #'model/islr-fp16-192-8-seed44-foldall-last.h5',\n",
    "               #'model/islr-fp16-192-8-seed45-foldall-last.h5',\n",
    "              ]\n",
    "models = [get_model() for _ in models_path]\n",
    "for model,path in zip(models,models_path):\n",
    "    model.load_weights(path)\n",
    "models[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:10:01.282095Z",
     "iopub.status.busy": "2024-11-19T09:10:01.281715Z",
     "iopub.status.idle": "2024-11-19T09:10:01.292456Z",
     "shell.execute_reply": "2024-11-19T09:10:01.291387Z",
     "shell.execute_reply.started": "2024-11-19T09:10:01.282057Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TFLiteModel(tf.Module):\n",
    "    \"\"\"\n",
    "    TensorFlow Lite model that takes input tensors and applies:\n",
    "        – a preprocessing model\n",
    "        – the ISLR model \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, islr_models):\n",
    "        \"\"\"\n",
    "        Initializes the TFLiteModel with the specified preprocessing model and ISLR model.\n",
    "        \"\"\"\n",
    "        super(TFLiteModel, self).__init__()\n",
    "\n",
    "        # Load the feature generation and main models\n",
    "        self.prep_inputs = Preprocess()\n",
    "        self.islr_models   = islr_models\n",
    "    \n",
    "    @tf.function(input_signature=[tf.TensorSpec(shape=[None, 543, 3], dtype=tf.float32, name='inputs')])\n",
    "    def __call__(self, inputs):\n",
    "        \"\"\"\n",
    "        Applies the feature generation model and main model to the input tensors.\n",
    "\n",
    "        Args:\n",
    "            inputs: Input tensor with shape [batch_size, 543, 3].\n",
    "\n",
    "        Returns:\n",
    "            A dictionary with a single key 'outputs' and corresponding output tensor.\n",
    "        \"\"\"\n",
    "        x = self.prep_inputs(tf.cast(inputs, dtype=tf.float32))\n",
    "        outputs = [model(x) for model in self.islr_models]\n",
    "        outputs = tf.keras.layers.Average()(outputs)[0]\n",
    "        return {'outputs': outputs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:10:01.294900Z",
     "iopub.status.busy": "2024-11-19T09:10:01.293906Z",
     "iopub.status.idle": "2024-11-19T09:10:01.304868Z",
     "shell.execute_reply": "2024-11-19T09:10:01.303872Z",
     "shell.execute_reply.started": "2024-11-19T09:10:01.294857Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ROWS_PER_FRAME = 543  # number of landmarks per frame\n",
    "def load_relevant_data_subset(pq_path):\n",
    "    data_columns = ['x', 'y', 'z']\n",
    "    data = pd.read_parquet('data/asl-signs/' + pq_path, columns=data_columns)\n",
    "    n_frames = int(len(data) / ROWS_PER_FRAME)\n",
    "    data = data.values.reshape(n_frames, ROWS_PER_FRAME, len(data_columns))\n",
    "    return data.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:10:01.308625Z",
     "iopub.status.busy": "2024-11-19T09:10:01.308199Z",
     "iopub.status.idle": "2024-11-19T09:10:09.606454Z",
     "shell.execute_reply": "2024-11-19T09:10:09.605429Z",
     "shell.execute_reply.started": "2024-11-19T09:10:01.308591Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D\n",
      "Z\n"
     ]
    }
   ],
   "source": [
    "tflite_keras_model = TFLiteModel(islr_models=models)\n",
    "demo_output = tflite_keras_model(load_relevant_data_subset(train_df.path[i]))[\"outputs\"]\n",
    "print(train_df.sign[i])\n",
    "print(decoder(np.argmax(demo_output.numpy(), axis=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:10:09.607930Z",
     "iopub.status.busy": "2024-11-19T09:10:09.607654Z",
     "iopub.status.idle": "2024-11-19T09:11:14.271634Z",
     "shell.execute_reply": "2024-11-19T09:11:14.270243Z",
     "shell.execute_reply.started": "2024-11-19T09:10:09.607906Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as preprocess_2_layer_call_fn, preprocess_2_layer_call_and_return_conditional_losses, 37_dwconv_dwconv_layer_call_fn, 37_dwconv_dwconv_layer_call_and_return_conditional_losses, _jit_compiled_convolution_op while saving (showing 5 of 202). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\syahr\\AppData\\Local\\Temp\\tmpnyzrrsig\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\syahr\\AppData\\Local\\Temp\\tmpnyzrrsig\\assets\n"
     ]
    }
   ],
   "source": [
    "keras_model_converter = tf.lite.TFLiteConverter.from_keras_model(tflite_keras_model)\n",
    "keras_model_converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "keras_model_converter.target_spec.supported_types = [tf.float16]\n",
    "tflite_model = keras_model_converter.convert()\n",
    "with open('model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:11:14.273865Z",
     "iopub.status.busy": "2024-11-19T09:11:14.273543Z",
     "iopub.status.idle": "2024-11-19T09:11:14.294253Z",
     "shell.execute_reply": "2024-11-19T09:11:14.293143Z",
     "shell.execute_reply.started": "2024-11-19T09:11:14.273833Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import ok\n",
      "    94400 / 94477  148 min 44 sec\n",
      "\n",
      "time_taken = 148 min 44 sec\n",
      "time_taken for LB = 94.463934 msec\n",
      "\n",
      "topk[0] = 0.9238438985149825\n",
      "topk[1] = 0.9573123617388359\n",
      "topk[2] = 0.9662563375213015\n",
      "topk[3] = 0.9708500481598696\n",
      "topk[4] = 0.97419477756491\n",
      "----- end -----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#check inference time\n",
    "#code from @hengck23\n",
    "mode = 'd' #'d'ebug #'s'ubmit\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "print('import ok')\n",
    "'''\n",
    "Your model must also require less than 40 MB in memory and \n",
    "perform inference with less than 100 milliseconds of latency per video. \n",
    "Expect to see approximately 40,000 videos in the test set. \n",
    "We allow an additional 10 minute buffer for loading the data and miscellaneous overhead.\n",
    "\n",
    "'''\n",
    "def time_to_str(t, mode='min'):\n",
    "    if mode=='min':\n",
    "        t  = int(t)/60\n",
    "        hr = t//60\n",
    "        min = t%60\n",
    "        return '%2d hr %02d min'%(hr,min)\n",
    "\n",
    "    elif mode=='sec':\n",
    "        t   = int(t)\n",
    "        min = t//60\n",
    "        sec = t%60\n",
    "        return '%2d min %02d sec'%(min,sec)\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "        \n",
    "ROWS_PER_FRAME = 543\n",
    "def load_relevant_data_subset(pq_path):\n",
    "    data_columns = ['x', 'y', 'z']\n",
    "    data = pd.read_parquet(pq_path, columns=data_columns)\n",
    "    n_frames = int(len(data) / ROWS_PER_FRAME)\n",
    "    data = data.values.reshape(n_frames, ROWS_PER_FRAME, len(data_columns))\n",
    "    return data.astype(np.float32)\n",
    "\n",
    "if mode in ['d']: \n",
    " \n",
    "    interpreter = tf.lite.Interpreter('model.tflite')\n",
    "    prediction_fn = interpreter.get_signature_runner('serving_default')\n",
    "    #valid_df = train_df[:1000]\n",
    "    valid_df = train_df\n",
    "    valid_num = len(valid_df)\n",
    "    valid = {\n",
    "        'sign':[],\n",
    "    }\n",
    "\n",
    "    start_timer = timer()\n",
    "    for t, d in valid_df.iterrows():\n",
    "\n",
    "        pq_file = f'data/asl-signs/{d.path}'\n",
    "        #print(pq_file)\n",
    "        xyz = load_relevant_data_subset(pq_file)\n",
    "\n",
    "        output = prediction_fn(inputs=xyz)\n",
    "        p = output['outputs'].reshape(-1)\n",
    "\n",
    "        valid['sign'].append(p)\n",
    "\n",
    "        #---\n",
    "        if t%100==0:\n",
    "            time_taken = timer() - start_timer\n",
    "            print('\\r %8d / %d  %s'%(t,valid_num,time_to_str(time_taken,'sec')),end='',flush=True)\n",
    "\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "    truth = valid_df.label.values\n",
    "    sign  = np.stack(valid['sign'])\n",
    "    predict = np.argsort(-sign, -1)\n",
    "    correct = predict==truth.reshape(valid_num,1)\n",
    "    topk = correct.cumsum(-1).mean(0)[:5]\n",
    "\n",
    "\n",
    "    print(f'time_taken = {time_to_str(time_taken,\"sec\")}')\n",
    "    print(f'time_taken for LB = {time_taken*1000/valid_num:05f} msec\\n')\n",
    "    for i in range(5):\n",
    "        print(f'topk[{i}] = {topk[i]}')  \n",
    "    print('----- end -----\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\syahr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2642: UserWarning: labels size, 250, does not match size of target_names, 94477\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        blow       0.96      0.92      0.94       385\n",
      "        wait       0.86      0.89      0.88       347\n",
      "       cloud       0.99      0.97      0.98       393\n",
      "        bird       0.95      0.91      0.93       386\n",
      "        owie       0.93      0.95      0.94       390\n",
      "        duck       0.94      0.89      0.91       348\n",
      "      minemy       0.89      0.90      0.89       370\n",
      "        lips       0.93      0.90      0.92       355\n",
      "      flower       0.97      0.96      0.96       377\n",
      "        time       0.85      0.92      0.88       362\n",
      "      vacuum       0.97      0.99      0.98       391\n",
      "       apple       0.69      0.44      0.54       401\n",
      "      puzzle       0.97      0.89      0.93       372\n",
      "      mitten       0.94      0.93      0.93       376\n",
      "       there       0.98      0.96      0.97       393\n",
      "         dry       0.88      0.90      0.89       351\n",
      "       shirt       0.98      0.95      0.96       395\n",
      "         owl       0.85      0.92      0.88       353\n",
      "      yellow       0.98      0.76      0.85       388\n",
      "        time       0.97      0.97      0.97       395\n",
      "         not       0.91      0.90      0.91       354\n",
      "      zipper       0.59      0.71      0.65       310\n",
      "       clean       0.94      0.94      0.94       376\n",
      "      closet       0.90      0.98      0.94       404\n",
      "       quiet       0.96      0.98      0.97       388\n",
      "        have       0.97      0.97      0.97       391\n",
      "     brother       0.94      0.96      0.95       362\n",
      "       clown       0.86      0.91      0.88       370\n",
      "       cheek       0.90      0.86      0.88       368\n",
      "        cute       0.94      0.97      0.96       382\n",
      "       store       0.98      0.95      0.96       395\n",
      "        shoe       1.00      0.99      0.99       403\n",
      "         wet       0.98      0.97      0.98       387\n",
      "       shirt       0.96      0.96      0.96       402\n",
      "         see       0.99      0.97      0.98       385\n",
      "       empty       0.83      0.90      0.86       366\n",
      "        fall       0.89      0.93      0.91       353\n",
      "     balloon       0.98      0.96      0.97       386\n",
      " frenchfries       0.89      0.93      0.91       400\n",
      "      finger       0.99      0.96      0.97       379\n",
      "        same       0.85      0.93      0.89       368\n",
      "         cry       0.95      0.95      0.95       375\n",
      "      hungry       0.85      0.86      0.85       348\n",
      "         owl       0.94      0.92      0.93       346\n",
      "      orange       0.96      0.97      0.97       386\n",
      "       cloud       0.80      0.92      0.86       355\n",
      "        milk       0.92      0.87      0.89       374\n",
      "          go       0.97      0.95      0.96       393\n",
      "       store       0.95      0.93      0.94       378\n",
      "      drawer       0.99      0.99      0.99       388\n",
      "          TV       0.95      0.97      0.96       404\n",
      "         dry       1.00      0.89      0.94       378\n",
      "        duck       0.97      0.93      0.95       390\n",
      "        blow       0.90      0.88      0.89       369\n",
      "     another       0.97      0.97      0.97       382\n",
      "     giraffe       0.98      0.93      0.95       378\n",
      "        wake       0.77      0.91      0.83       312\n",
      "         bee       0.97      0.94      0.95       365\n",
      "         bad       0.91      0.93      0.92       380\n",
      "         can       0.98      0.97      0.98       400\n",
      "      flower       0.98      0.96      0.97       410\n",
      "         say       0.85      0.92      0.88       327\n",
      " callonphone       0.91      0.92      0.92       373\n",
      "      finish       0.99      0.96      0.98       400\n",
      "         bee       0.89      0.94      0.91       356\n",
      "         old       0.90      0.94      0.92       391\n",
      "    backyard       0.97      0.85      0.90       369\n",
      "        sick       0.84      0.88      0.86       405\n",
      "        look       0.82      0.94      0.88       389\n",
      "        that       0.95      0.94      0.94       382\n",
      "       black       0.96      0.93      0.95       372\n",
      "    yourself       0.96      0.93      0.94       368\n",
      "        open       0.97      0.96      0.97       395\n",
      "   alligator       0.94      0.93      0.93       370\n",
      "         wet       0.90      0.83      0.86       382\n",
      "      closet       0.97      0.96      0.97       397\n",
      "        moon       0.95      0.91      0.93       362\n",
      "        find       0.97      0.84      0.90       372\n",
      "       pizza       0.99      0.96      0.97       397\n",
      "        shhh       0.89      0.91      0.90       360\n",
      "        fast       0.98      0.92      0.95       379\n",
      "      jacket       0.94      0.95      0.95       388\n",
      "    scissors       0.99      0.96      0.97       402\n",
      "         now       0.97      0.94      0.96       398\n",
      "          TV       0.87      0.92      0.89       380\n",
      "        wake       0.93      0.92      0.93       386\n",
      "         man       0.98      0.99      0.98       396\n",
      "      sticky       0.92      0.96      0.94       392\n",
      "        jump       0.97      0.96      0.96       384\n",
      "       sleep       0.95      0.95      0.95       384\n",
      "         sun       0.97      0.98      0.97       396\n",
      "       first       0.95      0.95      0.95       371\n",
      "      yellow       0.76      0.97      0.85       393\n",
      "     brother       0.97      0.93      0.95       382\n",
      "       grass       0.95      0.96      0.96       378\n",
      "       uncle       0.74      0.60      0.66       346\n",
      "        fish       0.93      0.89      0.91       390\n",
      "    scissors       0.92      0.89      0.90       388\n",
      "      cowboy       0.86      0.73      0.79       394\n",
      "        snow       0.92      0.95      0.93       382\n",
      "       dryer       0.96      0.95      0.95       387\n",
      "       green       0.96      0.95      0.96       386\n",
      "         bug       0.92      0.94      0.93       382\n",
      "         nap       0.97      0.96      0.97       390\n",
      "        feet       0.96      0.96      0.96       385\n",
      "       yucky       0.95      0.94      0.94       379\n",
      "     morning       0.89      0.89      0.89       342\n",
      "     brother       0.87      0.91      0.89       358\n",
      "         sad       0.86      0.91      0.88       337\n",
      "        face       0.95      0.88      0.91       348\n",
      "       penny       0.96      0.95      0.95       386\n",
      "      mitten       0.91      0.79      0.85       405\n",
      "          go       0.96      0.93      0.95       379\n",
      "        gift       0.94      0.96      0.95       390\n",
      "        open       0.99      0.95      0.97       377\n",
      "       night       0.88      0.90      0.89       357\n",
      "        hair       0.84      0.91      0.87       363\n",
      "         who       0.96      0.94      0.95       375\n",
      "       think       0.97      0.98      0.97       395\n",
      "        snow       0.98      0.97      0.97       391\n",
      "        look       0.97      0.95      0.96       383\n",
      "        look       0.93      0.94      0.94       385\n",
      "       brown       0.97      0.98      0.98       398\n",
      "         mad       0.99      0.95      0.97       366\n",
      "         bed       0.76      0.91      0.83       364\n",
      "        cute       0.79      0.94      0.85       336\n",
      "       drink       0.85      0.86      0.86       382\n",
      "        stay       0.94      0.94      0.94       383\n",
      "         bug       0.94      0.88      0.91       396\n",
      "        flag       0.96      0.86      0.91       389\n",
      "       tooth       0.97      0.93      0.95       370\n",
      "       awake       0.97      0.96      0.97       377\n",
      "    thankyou       0.95      0.93      0.94       376\n",
      "         hot       0.97      0.97      0.97       392\n",
      "       brown       0.85      0.89      0.87       402\n",
      "        like       0.96      0.93      0.94       415\n",
      "       where       0.98      0.90      0.94       414\n",
      "     hesheit       0.97      0.92      0.95       389\n",
      "       potty       0.99      0.97      0.98       392\n",
      "        time       0.97      0.95      0.96       398\n",
      "        down       0.97      0.94      0.95       392\n",
      "         old       0.95      0.96      0.95       377\n",
      "        sick       0.96      0.95      0.96       377\n",
      "       stuck       0.95      0.91      0.93       376\n",
      "          no       0.99      0.92      0.95       377\n",
      "        head       0.95      0.95      0.95       395\n",
      "        down       0.98      0.97      0.98       380\n",
      "      orange       0.93      0.94      0.94       372\n",
      "        food       0.93      0.97      0.95       408\n",
      "         sad       0.90      0.82      0.86       391\n",
      "      hungry       0.95      0.76      0.84       391\n",
      "       apple       0.97      0.94      0.96       398\n",
      "        sick       0.86      0.89      0.87       336\n",
      "     morning       0.92      0.97      0.94       370\n",
      "        stay       0.92      0.93      0.92       361\n",
      "      pretty       0.98      0.97      0.97       382\n",
      "        owie       0.96      0.93      0.94       377\n",
      "        flag       0.97      0.95      0.96       369\n",
      "        nuts       0.95      0.97      0.96       402\n",
      "        face       0.95      0.96      0.96       389\n",
      "      animal       0.79      0.90      0.84       368\n",
      "       clean       0.92      0.86      0.89       354\n",
      "      finger       0.98      0.98      0.98       390\n",
      "       green       0.93      0.93      0.93       386\n",
      " callonphone       0.95      0.89      0.92       365\n",
      "         say       0.99      0.97      0.98       396\n",
      "        frog       0.98      0.90      0.94       389\n",
      "      finish       0.78      0.69      0.73       399\n",
      "         owl       0.73      0.79      0.76       392\n",
      "      beside       0.96      0.94      0.95       390\n",
      "        bird       0.89      0.88      0.89       312\n",
      "       noisy       0.95      0.96      0.96       368\n",
      "       water       0.98      0.97      0.98       396\n",
      "        weus       0.88      0.96      0.92       384\n",
      "       happy       0.97      0.96      0.97       388\n",
      "       stuck       0.86      0.93      0.90       363\n",
      "         bug       0.92      0.93      0.92       351\n",
      "         say       0.98      0.90      0.94       404\n",
      "      sticky       0.96      0.95      0.95       392\n",
      "        owie       0.93      0.84      0.89       357\n",
      "       white       0.82      0.90      0.86       342\n",
      "         bye       0.93      0.92      0.93       358\n",
      "        high       0.98      0.98      0.98       387\n",
      "        fine       0.93      0.95      0.94       381\n",
      "       shirt       0.89      0.90      0.90       362\n",
      "      finish       0.93      0.95      0.94       379\n",
      "        boat       0.98      0.95      0.96       363\n",
      "         all       0.90      0.89      0.90       347\n",
      "       tiger       0.81      0.89      0.85       372\n",
      "        duck       0.99      0.96      0.98       391\n",
      "      pencil       0.95      0.92      0.93       380\n",
      "      sleepy       0.91      0.94      0.92       377\n",
      "        moon       0.91      0.92      0.91       369\n",
      "       tooth       0.95      0.97      0.96       388\n",
      "      puzzle       0.98      0.99      0.98       411\n",
      "     grandma       0.87      0.95      0.91       352\n",
      "   chocolate       0.83      0.95      0.88       364\n",
      "   alligator       0.96      0.94      0.95       372\n",
      "         not       0.98      0.96      0.97       380\n",
      "      haveto       0.71      0.86      0.78       380\n",
      "        down       0.87      0.85      0.86       403\n",
      "         cry       0.97      0.96      0.97       376\n",
      "      pretty       0.99      0.93      0.96       381\n",
      "        nuts       0.98      0.97      0.97       386\n",
      "        lips       0.97      0.91      0.94       382\n",
      "        down       0.84      0.88      0.86       376\n",
      "      jacket       0.89      0.88      0.88       349\n",
      "       radio       0.91      0.95      0.93       374\n",
      "        farm       0.97      0.93      0.95       369\n",
      "         bug       0.97      0.96      0.97       346\n",
      "         any       0.91      0.94      0.93       389\n",
      "        feet       0.75      0.92      0.82       337\n",
      "       zebra       0.98      0.94      0.96       394\n",
      "     giraffe       0.98      0.97      0.97       392\n",
      "      sleepy       0.94      0.96      0.95       357\n",
      "        rain       0.88      0.92      0.90       381\n",
      "         toy       0.90      0.79      0.84       363\n",
      "         cry       0.97      0.94      0.95       399\n",
      "        blow       0.88      0.95      0.92       360\n",
      "        fish       0.99      0.97      0.98       394\n",
      "      donkey       0.87      0.91      0.89       363\n",
      "       empty       0.96      0.95      0.95       388\n",
      "         bed       0.96      0.94      0.95       374\n",
      "        lion       0.91      0.89      0.90       393\n",
      "         dry       0.96      0.96      0.96       402\n",
      "         nap       0.94      0.94      0.94       372\n",
      "      pretty       0.95      0.94      0.95       378\n",
      "          no       0.95      0.92      0.93       382\n",
      "       sleep       0.99      0.99      0.99       405\n",
      "    yourself       0.98      0.90      0.94       373\n",
      "        gift       0.95      0.96      0.95       394\n",
      "         hot       0.60      0.81      0.69       307\n",
      "        drop       0.94      0.95      0.95       346\n",
      "     balloon       0.59      0.78      0.67       401\n",
      "        many       0.98      0.98      0.98       384\n",
      "        food       0.92      0.93      0.93       382\n",
      "        bath       0.87      0.91      0.89       371\n",
      "         wet       0.93      0.97      0.95       372\n",
      "        lips       0.96      0.95      0.96       370\n",
      "         dry       0.97      0.97      0.97       403\n",
      "        aunt       0.98      0.94      0.96       364\n",
      "        drop       0.92      0.94      0.93       369\n",
      "        time       0.98      0.95      0.97       388\n",
      "      sticky       0.99      0.96      0.98       384\n",
      "       tiger       0.92      0.90      0.91       386\n",
      "        weus       0.97      0.97      0.97       398\n",
      "          TV       0.94      0.93      0.93       385\n",
      "         not       0.94      0.86      0.90       379\n",
      "       sleep       0.91      0.93      0.92       375\n",
      "        boat       0.66      0.85      0.74       299\n",
      "\n",
      "    accuracy                           0.92     94477\n",
      "   macro avg       0.93      0.92      0.92     94477\n",
      "weighted avg       0.93      0.92      0.92     94477\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "report = classification_report(truth, np.argmax(sign, axis=1), \n",
    "                               labels = np.arange(250),\n",
    "                               target_names=valid_df['sign'].values)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:11:14.296064Z",
     "iopub.status.busy": "2024-11-19T09:11:14.295365Z",
     "iopub.status.idle": "2024-11-19T09:11:14.390602Z",
     "shell.execute_reply": "2024-11-19T09:11:14.389539Z",
     "shell.execute_reply.started": "2024-11-19T09:11:14.296037Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['TV', 'after', 'airplane', 'all', 'alligator', 'animal', 'another',\n",
       "       'any', 'apple', 'arm', 'aunt', 'awake', 'backyard', 'bad',\n",
       "       'balloon', 'bath', 'because', 'bed', 'bedroom', 'bee', 'before',\n",
       "       'beside', 'better', 'bird', 'black', 'blow', 'blue', 'boat',\n",
       "       'book', 'boy', 'brother', 'brown', 'bug', 'bye', 'callonphone',\n",
       "       'can', 'car', 'carrot', 'cat', 'cereal', 'chair', 'cheek', 'child',\n",
       "       'chin', 'chocolate', 'clean', 'close', 'closet', 'cloud', 'clown',\n",
       "       'cow', 'cowboy', 'cry', 'cut', 'cute', 'dad', 'dance', 'dirty',\n",
       "       'dog', 'doll', 'donkey', 'down', 'drawer', 'drink', 'drop', 'dry',\n",
       "       'dryer', 'duck', 'ear', 'elephant', 'empty', 'every', 'eye',\n",
       "       'face', 'fall', 'farm', 'fast', 'feet', 'find', 'fine', 'finger',\n",
       "       'finish', 'fireman', 'first', 'fish', 'flag', 'flower', 'food',\n",
       "       'for', 'frenchfries', 'frog', 'garbage', 'gift', 'giraffe', 'girl',\n",
       "       'give', 'glasswindow', 'go', 'goose', 'grandma', 'grandpa',\n",
       "       'grass', 'green', 'gum', 'hair', 'happy', 'hat', 'hate', 'have',\n",
       "       'haveto', 'head', 'hear', 'helicopter', 'hello', 'hen', 'hesheit',\n",
       "       'hide', 'high', 'home', 'horse', 'hot', 'hungry', 'icecream', 'if',\n",
       "       'into', 'jacket', 'jeans', 'jump', 'kiss', 'kitty', 'lamp',\n",
       "       'later', 'like', 'lion', 'lips', 'listen', 'look', 'loud', 'mad',\n",
       "       'make', 'man', 'many', 'milk', 'minemy', 'mitten', 'mom', 'moon',\n",
       "       'morning', 'mouse', 'mouth', 'nap', 'napkin', 'night', 'no',\n",
       "       'noisy', 'nose', 'not', 'now', 'nuts', 'old', 'on', 'open',\n",
       "       'orange', 'outside', 'owie', 'owl', 'pajamas', 'pen', 'pencil',\n",
       "       'penny', 'person', 'pig', 'pizza', 'please', 'police', 'pool',\n",
       "       'potty', 'pretend', 'pretty', 'puppy', 'puzzle', 'quiet', 'radio',\n",
       "       'rain', 'read', 'red', 'refrigerator', 'ride', 'room', 'sad',\n",
       "       'same', 'say', 'scissors', 'see', 'shhh', 'shirt', 'shoe',\n",
       "       'shower', 'sick', 'sleep', 'sleepy', 'smile', 'snack', 'snow',\n",
       "       'stairs', 'stay', 'sticky', 'store', 'story', 'stuck', 'sun',\n",
       "       'table', 'talk', 'taste', 'thankyou', 'that', 'there', 'think',\n",
       "       'thirsty', 'tiger', 'time', 'tomorrow', 'tongue', 'tooth',\n",
       "       'toothbrush', 'touch', 'toy', 'tree', 'uncle', 'underwear', 'up',\n",
       "       'vacuum', 'wait', 'wake', 'water', 'wet', 'weus', 'where', 'white',\n",
       "       'who', 'why', 'will', 'wolf', 'yellow', 'yes', 'yesterday',\n",
       "       'yourself', 'yucky', 'zebra', 'zipper'], dtype=object)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.sort_values(\"sign\")[\"sign\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 5087314,
     "sourceId": 46105,
     "sourceType": "competition"
    },
    {
     "datasetId": 3221731,
     "sourceId": 5600436,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30408,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
