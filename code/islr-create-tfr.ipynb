{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f36bd9b",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-03-23T16:03:52.514176Z",
     "iopub.status.busy": "2023-03-23T16:03:52.513519Z",
     "iopub.status.idle": "2023-03-23T16:04:02.623639Z",
     "shell.execute_reply": "2023-03-23T16:04:02.622353Z"
    },
    "papermill": {
     "duration": 10.121234,
     "end_time": "2023-03-23T16:04:02.626434",
     "exception": false,
     "start_time": "2023-03-23T16:03:52.505200",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import cpu_count\n",
    "from sklearn.model_selection import StratifiedGroupKFold, KFold\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fc48d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'J': 9, 'K': 10, 'L': 11, 'M': 12, 'N': 13, 'O': 14, 'P': 15, 'Q': 16, 'R': 17, 'S': 18, 'T': 19, 'U': 20, 'V': 21, 'W': 22, 'X': 23, 'Y': 24, 'Z': 25, 'TV': 26, 'after': 27, 'airplane': 28, 'all': 29, 'alligator': 30, 'animal': 31, 'another': 32, 'any': 33, 'apple': 34, 'arm': 35, 'aunt': 36, 'awake': 37, 'backyard': 38, 'bad': 39, 'balloon': 40, 'bath': 41, 'because': 42, 'bed': 43, 'bedroom': 44, 'bee': 45, 'before': 46, 'beside': 47, 'better': 48, 'bird': 49, 'black': 50, 'blow': 51, 'blue': 52, 'boat': 53, 'book': 54, 'boy': 55, 'brother': 56, 'brown': 57, 'bug': 58, 'bye': 59, 'callonphone': 60, 'can': 61, 'car': 62, 'carrot': 63, 'cat': 64, 'cereal': 65, 'chair': 66, 'cheek': 67, 'child': 68, 'chin': 69, 'chocolate': 70, 'clean': 71, 'close': 72, 'closet': 73, 'cloud': 74, 'clown': 75, 'cow': 76, 'cowboy': 77, 'cry': 78, 'cut': 79, 'cute': 80, 'dad': 81, 'dance': 82, 'dirty': 83, 'dog': 84, 'doll': 85, 'donkey': 86, 'down': 87, 'drawer': 88, 'drink': 89, 'drop': 90, 'dry': 91, 'dryer': 92, 'duck': 93, 'ear': 94, 'elephant': 95, 'empty': 96, 'every': 97, 'eye': 98, 'face': 99, 'fall': 100, 'farm': 101, 'fast': 102, 'feet': 103, 'find': 104, 'fine': 105, 'finger': 106, 'finish': 107, 'fireman': 108, 'first': 109, 'fish': 110, 'flag': 111, 'flower': 112, 'food': 113, 'for': 114, 'frenchfries': 115, 'frog': 116, 'garbage': 117, 'gift': 118, 'giraffe': 119, 'girl': 120, 'give': 121, 'glasswindow': 122, 'go': 123, 'goose': 124, 'grandma': 125, 'grandpa': 126, 'grass': 127, 'green': 128, 'gum': 129, 'hair': 130, 'happy': 131, 'hat': 132, 'hate': 133, 'have': 134, 'haveto': 135, 'head': 136, 'hear': 137, 'helicopter': 138, 'hello': 139, 'hen': 140, 'hesheit': 141, 'hide': 142, 'high': 143, 'home': 144, 'horse': 145, 'hot': 146, 'hungry': 147, 'icecream': 148, 'if': 149, 'into': 150, 'jacket': 151, 'jeans': 152, 'jump': 153, 'kiss': 154, 'kitty': 155, 'lamp': 156, 'later': 157, 'like': 158, 'lion': 159, 'lips': 160, 'listen': 161, 'look': 162, 'loud': 163, 'mad': 164, 'make': 165, 'man': 166, 'many': 167, 'milk': 168, 'minemy': 169, 'mitten': 170, 'mom': 171, 'moon': 172, 'morning': 173, 'mouse': 174, 'mouth': 175, 'nap': 176, 'napkin': 177, 'night': 178, 'no': 179, 'noisy': 180, 'nose': 181, 'not': 182, 'now': 183, 'nuts': 184, 'old': 185, 'on': 186, 'open': 187, 'orange': 188, 'outside': 189, 'owie': 190, 'owl': 191, 'pajamas': 192, 'pen': 193, 'pencil': 194, 'penny': 195, 'person': 196, 'pig': 197, 'pizza': 198, 'please': 199, 'police': 200, 'pool': 201, 'potty': 202, 'pretend': 203, 'pretty': 204, 'puppy': 205, 'puzzle': 206, 'quiet': 207, 'radio': 208, 'rain': 209, 'read': 210, 'red': 211, 'refrigerator': 212, 'ride': 213, 'room': 214, 'sad': 215, 'same': 216, 'say': 217, 'scissors': 218, 'see': 219, 'shhh': 220, 'shirt': 221, 'shoe': 222, 'shower': 223, 'sick': 224, 'sleep': 225, 'sleepy': 226, 'smile': 227, 'snack': 228, 'snow': 229, 'stairs': 230, 'stay': 231, 'sticky': 232, 'store': 233, 'story': 234, 'stuck': 235, 'sun': 236, 'table': 237, 'talk': 238, 'taste': 239, 'thankyou': 240, 'that': 241, 'there': 242, 'think': 243, 'thirsty': 244, 'tiger': 245, 'time': 246, 'tomorrow': 247, 'tongue': 248, 'tooth': 249, 'toothbrush': 250, 'touch': 251, 'toy': 252, 'tree': 253, 'uncle': 254, 'underwear': 255, 'up': 256, 'vacuum': 257, 'wait': 258, 'wake': 259, 'water': 260, 'wet': 261, 'weus': 262, 'where': 263, 'white': 264, 'who': 265, 'why': 266, 'will': 267, 'wolf': 268, 'yellow': 269, 'yes': 270, 'yesterday': 271, 'yourself': 272, 'yucky': 273, 'zebra': 274, 'zipper': 275}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>participant_id</th>\n",
       "      <th>sequence_id</th>\n",
       "      <th>sign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_landmark_files\\1\\A_asl-alphabet_asl_alph...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_landmark_files\\1\\A_asl-alphabet_asl_alph...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_landmark_files\\1\\A_asl-alphabet_asl_alph...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_landmark_files\\1\\A_asl-alphabet_asl_alph...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_landmark_files\\1\\A_asl-alphabet_asl_alph...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39241</th>\n",
       "      <td>train_landmark_files\\1\\Z_asl-sign-language-alp...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39242</th>\n",
       "      <td>train_landmark_files\\1\\Z_asl-sign-language-alp...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39243</th>\n",
       "      <td>train_landmark_files\\1\\Z_asl-sign-language-alp...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39244</th>\n",
       "      <td>train_landmark_files\\1\\Z_asl-sign-language-alp...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39245</th>\n",
       "      <td>train_landmark_files\\1\\Z_asl-sign-language-alp...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39246 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    path  participant_id  \\\n",
       "0      train_landmark_files\\1\\A_asl-alphabet_asl_alph...               1   \n",
       "1      train_landmark_files\\1\\A_asl-alphabet_asl_alph...               1   \n",
       "2      train_landmark_files\\1\\A_asl-alphabet_asl_alph...               1   \n",
       "3      train_landmark_files\\1\\A_asl-alphabet_asl_alph...               1   \n",
       "4      train_landmark_files\\1\\A_asl-alphabet_asl_alph...               1   \n",
       "...                                                  ...             ...   \n",
       "39241  train_landmark_files\\1\\Z_asl-sign-language-alp...               1   \n",
       "39242  train_landmark_files\\1\\Z_asl-sign-language-alp...               1   \n",
       "39243  train_landmark_files\\1\\Z_asl-sign-language-alp...               1   \n",
       "39244  train_landmark_files\\1\\Z_asl-sign-language-alp...               1   \n",
       "39245  train_landmark_files\\1\\Z_asl-sign-language-alp...               1   \n",
       "\n",
       "       sequence_id sign  \n",
       "0              0.0    A  \n",
       "1              0.0    A  \n",
       "2              0.0    A  \n",
       "3              0.0    A  \n",
       "4              0.0    A  \n",
       "...            ...  ...  \n",
       "39241          0.0    Z  \n",
       "39242          0.0    Z  \n",
       "39243          0.0    Z  \n",
       "39244          0.0    Z  \n",
       "39245          0.0    Z  \n",
       "\n",
       "[39246 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "train_df = train_df.loc[(train_df[\"sequence_id\"] == 0)].reset_index(drop = True)\n",
    "with open('sign_to_prediction_index_map_merge.json') as json_file:\n",
    "    LABEL_DICT = json.load(json_file)\n",
    "    \n",
    "print(LABEL_DICT)\n",
    "#train_df = train_df.loc[(train_df[\"sign\"].str.len() == 1)].reset_index(drop=True)\n",
    "\n",
    "# Randomly select 5 rows for each unique \"sign\"\n",
    "#train_df = train_df.groupby(\"sign\").apply(lambda x: x.sample(n=min(25, len(x)))).reset_index(drop=True)\n",
    "\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e298000",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-23T16:04:03.156305Z",
     "iopub.status.busy": "2023-03-23T16:04:03.155886Z",
     "iopub.status.idle": "2023-03-23T16:04:03.162947Z",
     "shell.execute_reply": "2023-03-23T16:04:03.161634Z"
    },
    "papermill": {
     "duration": 0.01705,
     "end_time": "2023-03-23T16:04:03.165482",
     "exception": false,
     "start_time": "2023-03-23T16:04:03.148432",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5fold training 39246 samples\n",
      "fold0: train 31396 valid 7850\n",
      "fold1: train 31397 valid 7849\n",
      "fold2: train 31397 valid 7849\n",
      "fold3: train 31397 valid 7849\n",
      "fold4: train 31397 valid 7849\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>participant_id</th>\n",
       "      <th>sequence_id</th>\n",
       "      <th>sign</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_landmark_files\\1\\A_asl-alphabet_asl_alph...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_landmark_files\\1\\A_asl-alphabet_asl_alph...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_landmark_files\\1\\A_asl-alphabet_asl_alph...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_landmark_files\\1\\A_asl-alphabet_asl_alph...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_landmark_files\\1\\A_asl-alphabet_asl_alph...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path  participant_id  \\\n",
       "0  train_landmark_files\\1\\A_asl-alphabet_asl_alph...               1   \n",
       "1  train_landmark_files\\1\\A_asl-alphabet_asl_alph...               1   \n",
       "2  train_landmark_files\\1\\A_asl-alphabet_asl_alph...               1   \n",
       "3  train_landmark_files\\1\\A_asl-alphabet_asl_alph...               1   \n",
       "4  train_landmark_files\\1\\A_asl-alphabet_asl_alph...               1   \n",
       "\n",
       "   sequence_id sign  fold  \n",
       "0          0.0    A     1  \n",
       "1          0.0    A     2  \n",
       "2          0.0    A     3  \n",
       "3          0.0    A     2  \n",
       "4          0.0    A     0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "def load_relevant_data_subset(pq_path):\n",
    "    data_columns = ['x', 'y', 'z']\n",
    "    data = pd.read_parquet(pq_path, columns=data_columns)\n",
    "    n_frames = int(len(data) / ROWS_PER_FRAME)\n",
    "    data = data.values.reshape(n_frames, ROWS_PER_FRAME, len(data_columns))\n",
    "    return data.astype(np.float32)\n",
    "\n",
    "def encode_row(row):\n",
    "    coordinates = load_relevant_data_subset(f'data/asl-signs/{row.path}')\n",
    "    coordinates_encoded = coordinates.tobytes()\n",
    "    participant_id = int(row.participant_id)\n",
    "    sequence_id = int(row.sequence_id)\n",
    "    sign = int(LABEL_DICT[row.sign])\n",
    "    record_bytes = tf.train.Example(features=tf.train.Features(feature={\n",
    "                'coordinates': tf.train.Feature(bytes_list=tf.train.BytesList(value=[coordinates_encoded])),\n",
    "                'participant_id': tf.train.Feature(int64_list=tf.train.Int64List(value=[participant_id])),\n",
    "                'sequence_id':tf.train.Feature(int64_list=tf.train.Int64List(value=[sequence_id])),\n",
    "                'sign':tf.train.Feature(int64_list=tf.train.Int64List(value=[sign])),\n",
    "                })).SerializeToString()\n",
    "    return record_bytes\n",
    "\n",
    "def process_chunk(chunk, tfrecord_name):\n",
    "    options = tf.io.TFRecordOptions(compression_type='GZIP', compression_level=9)\n",
    "    with tf.io.TFRecordWriter(tfrecord_name, options=options) as file_writer:\n",
    "        for i, row in tqdm(chunk.iterrows()):\n",
    "            record_bytes = encode_row(row)\n",
    "            file_writer.write(record_bytes)\n",
    "            del record_bytes\n",
    "        file_writer.close()\n",
    "\n",
    "ROWS_PER_FRAME = 543\n",
    "N_FILES = len(train_df)\n",
    "CHUNK_SIZE = 512\n",
    "N_PART = 1\n",
    "FOLD = 4\n",
    "part = 0\n",
    "\n",
    "class CFG:\n",
    "    seed = 42\n",
    "    n_splits = 5\n",
    "\n",
    "row = train_df.iloc[0]\n",
    "coordinates = load_relevant_data_subset(f'data/asl-signs/{row.path}')\n",
    "coordinates_encoded = coordinates.tobytes()\n",
    "participant_id = int(row.participant_id)\n",
    "sequence_id = int(row.sequence_id) if pd.notna(row.sequence_id) else 0\n",
    "sign = int(LABEL_DICT[row.sign])\n",
    "\n",
    "record_bytes = tf.train.Example(features=tf.train.Features(feature={\n",
    "            'coordinates': tf.train.Feature(bytes_list=tf.train.BytesList(value=[coordinates_encoded])),\n",
    "            'participant_id': tf.train.Feature(int64_list=tf.train.Int64List(value=[participant_id])),\n",
    "            'sequence_id':tf.train.Feature(int64_list=tf.train.Int64List(value=[sequence_id])),\n",
    "            'sign':tf.train.Feature(int64_list=tf.train.Int64List(value=[sign])),\n",
    "            }))\n",
    "\n",
    "train_folds = train_df.copy()\n",
    "train_folds['fold']=-1\n",
    "\n",
    "num_bins = 5\n",
    "\n",
    "# train_folds = train_folds.sample(frac=1, random_state=CFG.seed).reset_index(drop=True)\n",
    "# gkfold = StratifiedGroupKFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.seed) \n",
    "# print(f'{CFG.n_splits}fold training', len(train_folds), 'samples')\n",
    "# for fold_idx, (train_idx, valid_idx) in enumerate(gkfold.split(train_folds, y=train_folds['sign'].values, groups=train_folds.participant_id)):\n",
    "#     train_folds.loc[valid_idx,'fold'] = fold_idx\n",
    "#     print(f'fold{fold_idx}:', 'train', len(train_idx), 'valid', len(valid_idx))\n",
    "kfold = KFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.seed) \n",
    "print(f'{CFG.n_splits}fold training', len(train_folds), 'samples')\n",
    "for fold_idx, (train_idx, valid_idx) in enumerate(kfold.split(train_folds)):\n",
    "    train_folds.loc[valid_idx,'fold'] = fold_idx\n",
    "    print(f'fold{fold_idx}:', 'train', len(train_idx), 'valid', len(valid_idx))\n",
    "    \n",
    "assert not (train_folds['fold']==-1).sum()\n",
    "assert len(np.unique(train_folds['fold']))==CFG.n_splits\n",
    "train_folds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db4df2fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-23T16:04:15.192582Z",
     "iopub.status.busy": "2023-03-23T16:04:15.192116Z",
     "iopub.status.idle": "2023-03-23T16:21:37.656209Z",
     "shell.execute_reply": "2023-03-23T16:21:37.654145Z"
    },
    "papermill": {
     "duration": 1042.476283,
     "end_time": "2023-03-23T16:21:37.659348",
     "exception": false,
     "start_time": "2023-03-23T16:04:15.183065",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Put every image in a seperate TFRecord file\n",
    "# Make Pairs of Views as input to the model\n",
    "\n",
    "def split_dataframe(df, chunk_size = 10000): \n",
    "    chunks = list()\n",
    "    num_chunks = len(df) // chunk_size + 1\n",
    "    for i in range(num_chunks):\n",
    "        chunks.append(df[i*chunk_size:(i+1)*chunk_size])\n",
    "    return chunks\n",
    "\n",
    "#DATASET_NAME = f'islr-{CFG.n_splits}fold'\n",
    "DATASET_NAME = f'fslr-{CFG.n_splits}fold'\n",
    "#DATASET_NAME = f'aslr-{CFG.n_splits}fold'\n",
    "\n",
    "for fold in range(CFG.n_splits):#[FOLD]:#range(CFG.n_splits):\n",
    "    rows = train_folds[train_folds['fold']==fold]\n",
    "    chunks = split_dataframe(rows, CHUNK_SIZE)\n",
    "    part_size = len(chunks)//N_PART\n",
    "    last = (part+1)*part_size if part != N_PART - 1 else len(chunks)+1\n",
    "    chunks = chunks[part*part_size:last]\n",
    "    \n",
    "    N = [len(x) for x in chunks]\n",
    "    _ = Parallel(n_jobs=cpu_count())(\n",
    "        delayed(process_chunk)(x, f'data/{DATASET_NAME}/fold{fold}-{i}-{n}.tfrecords')\n",
    "        for i,(x,n) in enumerate(zip(chunks,N))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1677b2d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2413.090372,
   "end_time": "2023-03-23T16:43:55.112208",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-03-23T16:03:42.021836",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
